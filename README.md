# AI & The Justice System

### Introduction
Human decision-making is, by nature, biased. Judges in all Justice Systems can suffer from numerous decision-altering afflictions in the form of cognitive biases and social biases (Shirodkar, 2021). Would an AI Judge remove bias and discrimination from the Justice System or does it carry with it the bias of its creators? (Bagaric et al., 2022, 95-148) 

### Ethical Arguments for the use of AI in the Justice System 
The use of AI in the Justice System can benefit society in a number of ways. In a supportive role, AI can allow for more cost and time-effective proceedings in the form of data analysis which would be impossible for a human equivalent in the same time frame. (Sourdin, 2018, 1114-1133) This supports the justice system allowing workers to focus on adjudicative tasks and accurately assessing client risk. This reliance on more factual data develops trust in the system for society as a whole. 
“... One cannot expect any tool to reverse centuries of racial injustice or gender inequality…” (Bagaric et al., 2022, 95-148), however an AI, if accurately trained, informed with an appropriate data set and a wide and unbiased ethical framework approach, could allow for consistent and unbiased decision-making processes and theoretically erase discrimination and bias. This would remove the human emotional element from the system to avoid life-changing decisions based on intuition or bias. (Sourdin, 2018, 1114-1133) 
An AI judge or supplement tool is currently held to standards of the Procedural European Commission for the Efficiency of Justice, and the European Ethical Charter on the use of Artificial Intelligence in the Judicial Systems and their Environment among other legislation (Noel, 2023). If applied correctly, an AI tool within the Justice System could vastly improve the accuracy and impartiality at all levels within the system. 


### Ethical Arguments Against the use of AI in the Justice System 
Machine learning algorithms allow continuous learning based on data, allowing a machine to rewrite and ‘improve’ upon its previous assumptions. This can cause machine bias itself, as it is choosing more ‘correct’ and uniform decision-making paths based on previous learning, disregarding the paths/data that seem irrelevant. (Bagaric et al., 2022, 95-148) This could create a cycle of biased selection based on the years of historical data from which it is pulled, creating skewed algorithm predictions and discrimination. This has been seen already with the US COMPAS algorithm, which has potentially unintentional racial biases (Noel, 2023). 
Do engineers legally have a right to teach machines the law itself? (Sourdin, 2018, 1114-1133) Legislation is sometimes not clear cut, and input of laws and rules is put to private developers and engineers. Machine learning informed by utility rather than individualised cases could have unjust outcomes for cases in certain categories (Sourdin, 2018, 1114-1133). 
The transparency and consistency of deep learning network outcomes is an issue with most algorithms manufactured by private companies (Bagaric et al., 2022, 94-148). For example, the COMPAS algorithm is considered a trade secret, not allowing public examination (Noel, 2023). 
Having a strong AI presence within any section of the Justice System could see the individual fall away from trust in the justice system with its lack of human intuition and oversight and its unstandardised ethical frameworks in which it is trained. An AI can currently only simulate human intelligence, it cannot replicate it. 


### Liability 
From data sets, coding, neural network node weight, private and public developers, installers of policy and governance of algorithm auditing, to lawyers and police and sentencers; who is liable for the mistakes of a man-made self-learning machine? 
A deontological approach would suggest that liability falls within the scope of current morals and public policy, if a private sector for example has not trained an AI to the current standards and its actions are transparent and at fault then they may hold liability. Whereas a utility-based approach may consider liability falls upon the entity that caused the most detrimental actions.  
The more prevalent question may be; how do we avoid the mistakes of an AI? The onus may fall to creators and engineers to code non-biased best practices and policies into their machines to avoid mistakes in the first place. (Bagaric et al., 2022, 95-148) Along with strict algorithm monitoring and auditing, correction of code and redesign of biased algorithms will be essential to create consistency in AI technology to aid the legal system and avoid AI mistakes in which liability will inevitably fall upon the highest governing body. (Bagaric et al., 2022, 95-148)


### Ethical Dilemmas in Predictive Policing 
AI Prediction technology in policing addresses areas where future crime is likely to arise. By using data sets pulled from criminal records of the court and police and historical data, algorithms analyse and assess risk areas to predict criminal acts and activity. 
This technology focuses on patrol geographies that often stem from historical data of over-policing minority communities creating a cycle of discrimination and bias that cannot be undone. Machine ‘data driven’ patrols may be misinterpreted by their human overseers based on previous hot-spot geographics (Shapiro, 2019, 456-472). Police attending these areas will all ready be biased towards the individuals and may be more likely to arrest or react discriminatorily based on knowledge fed to them by the AI. (Bagaric et al., 2022, 95-148)
Risk assessment tools should be transparent in their nature and in the decisions of the weight of each interest to ensure the least amount of bias based on hard data.

### Conclusions
At this moment in time, the role of AI in deciding guilt (Noel, 2023) should be continuously  scrutinized. There are many opportunities for AI tools in assisting the justice system, it should not, however, entirely replace human intelligence in judicial decision-making. AI should be applied as transhumanism: “fundamentally improving the human condition through applied reason… to greatly enhance human intellectual… capacities” (Sourdin, 2018, 1114-1133) to be used as a tool rather than a replacement. There will need to be a slow and steady switch of trust towards AI in the role of high-stakes decision-making in the Justice System as well as education in its opportunities and reward as well as careful oversight from human intelligence (Noel, 2023). 


### References
Bagaric, M., Svilar, J., Bull, M., Hunter, D., & Stobbs, N. (2022, 12 1). The solution to the pervasive bias and discrimination in the criminal justice system: transparent and fair artificial intelligence. The American Criminal Law Review, 59(1), 95-148.

Noel, M. (2023, May 22). AI is already being used in the legal system – we need to pay more attention to how we use it. The Conversation. Retrieved July 30, 2023, from https://theconversation.com/ai-is-already-being-used-in-the-legal-system-we-need-to-pay-more-attention-to-how-we-use-it-205441

Shapiro, A. (2019). Predictive Policing for Reform? Indeterminacy and Intervention in Big Data Policing. Surveillance & Society, 17(3/4), 456-472. https://doi.org/10.24908/ss.v17i3/4.10410

Shirodkar, S. (2021, April). JUDICIAL. JUDICIAL. Retrieved July 30, 2023, from https://www.alrc.gov.au/wp-content/uploads/2021/04/JI6-Cognitive-Biases-in-Judicial-Decision-Making.pdf

Sourdin, T. (2018). Judge v Robot?: Artificial intelligence and judicial decision-making. University of New South Wales Law Journal, 41(4), 1114-1133. https://doi.org/10.53637/ZGUX2213
